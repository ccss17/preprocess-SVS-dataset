{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import whisper\n",
    "from rich import print as rprint\n",
    "import midii\n",
    "\n",
    "import preprocess_svs as ps\n",
    "from preprocess_svs import gv\n",
    "from preprocess_svs import LyricNormalizer, SVS_Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GV File Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv_path = \"D:/dataset/177.다음색 가이드보컬 데이터\"\n",
    "gv_json_sample = \"sample/gv/json\"\n",
    "gv_mid_sample = \"sample/gv/midi\"\n",
    "gv_sample_preprocessed = \"sample/gv/json_preprocessed\"\n",
    "gv_json_time_adjusted = \"D:/dataset/다음색 가이드보컬 데이터 time_adjusted\"\n",
    "gv_json_preprocessed = \"D:/dataset/다음색 가이드보컬 데이터 json preprocessed\"\n",
    "midi_filepath = \"sample/gv/midi/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.mid\"\n",
    "time_adjusted_json_filepath = \"sample/gv/json_time_adjusted/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\"\n",
    "filled_time_gaps_json_filepath = \"sample/gv/json_filled_time_gaps/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(list(ps.get_files(gv_path, \"mid\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(857142, 790)]\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "mid = midii.MidiFile(midi_filepath, convert_1_to_0=True)\n",
    "tempo_rank = mid.tempo_rank()\n",
    "print(tempo_rank)\n",
    "print(ps.calculate_top_tempo_percentage(tempo_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Tempo Deviation\n",
    "\n",
    "- json 을 처리하려면 quantize 를 위한 tempo 가 필요한데 json 에는 tempo 정보가 없음 \n",
    "- -> tempo rank 검사 \n",
    "- -> tempo 가 변하지 않는다는 충분한 보장\n",
    "- -> dominate tempo 를 채택하여 quantize 해도 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps.tempo_statistics(gv_path, parallel=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- -> 이전 end_time 이 현재 start_time 보다 큰 경우가 있음 \n",
    "- -> 이전 end_time 에 현재 start_time 을 맞추면, 뒤따라오는 메시지들의 sync 가 다 틀어짐 \n",
    "- -> 이전 end_time 을 현재 start_time 에 맞춰주는 게 더 나음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify notes sorted by time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gv.verify_json_notes_sorted_by_time(gv_path, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_note_times_sample():\n",
    "    gv_path = \"sample/gv/json\"\n",
    "    for json_path in ps.get_files(gv_path, \"json\"):\n",
    "        p_orig = Path(json_path)\n",
    "        out_path = p_orig.parent.parent / \"json_time_adjusted\" / p_orig.name\n",
    "        out_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        print(f\"adjust time of \\n{json_path}\")\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        notes = data.get(\"notes\")\n",
    "        processed_notes = gv.adjust_note_times(notes)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(processed_notes, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"saved to \\n{out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjust time of \n",
      "sample/gv/json/SINGER_66_30TO49_HUSKY_MALE_DANCE_C2835.json\n",
      "saved to \n",
      "sample/gv/json_time_adjusted/SINGER_66_30TO49_HUSKY_MALE_DANCE_C2835.json\n",
      "adjust time of \n",
      "sample/gv/json/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n",
      "saved to \n",
      "sample/gv/json_time_adjusted/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n"
     ]
    }
   ],
   "source": [
    "adjust_note_times_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill silence note between notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample/gv/json_time_adjusted/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n",
      "sample/gv/json_filled_time_gaps/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n"
     ]
    }
   ],
   "source": [
    "print(time_adjusted_json_filepath)\n",
    "print(filled_time_gaps_json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save:\n",
      "sample/gv/json_filled_time_gaps/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n"
     ]
    }
   ],
   "source": [
    "gv.fill_time_gaps_save(time_adjusted_json_filepath, filled_time_gaps_json_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify correspondence json vs wav vs mid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">([]</span>, <span style=\"font-weight: bold\">[])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">([]</span>, <span style=\"font-weight: bold\">[])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">([]</span>, <span style=\"font-weight: bold\">[])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jsons = ps.get_files(gv_path, \"json\", sort=True)\n",
    "mids = ps.get_files(gv_path, \"mid\", sort=True)\n",
    "wavs = ps.get_files(gv_path, \"wav\", sort=True)\n",
    "rprint(gv.verify_files_coherent(jsons, mids))\n",
    "rprint(gv.verify_files_coherent(wavs, mids))\n",
    "rprint(gv.verify_files_coherent(jsons, wavs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove abnormal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gv.remove_abnormal_file(gv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GV Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filepath = \"sample/gv/json_preprocessed/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\"\n",
    "split_json_filepath = \"sample/gv/split_json/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\"\n",
    "preprocessed_gv_path = \"preprocessed_gv/\"\n",
    "preprocessed_gv_duration_path = \"preprocessed_gv/duration\"\n",
    "preprocessed_gv_pitch_path = \"preprocessed_gv/pitch\"\n",
    "preprocessed_gv_wav_path = \"preprocessed_gv/wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - preprocess gv json\n",
    "\n",
    "- gv json -> adjust note times + fill time gaps + quantization + frames\n",
    "- embed coherent json format(sharing with mssv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample/gv/json sample/gv/midi sample/gv/json_preprocessed\n"
     ]
    }
   ],
   "source": [
    "print(gv_json_sample, gv_mid_sample, gv_sample_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.preprocess_json(\n",
    "    gv_json_sample, gv_mid_sample, gv_sample_preprocessed, parallel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gv.preprocess_json(\n",
    "#     gv_path,\n",
    "#     gv_path,\n",
    "#     gv_json_preprocessed,\n",
    "#     parallel=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - split notes by silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_json = ps.split_json_by_silence(json_filepath, min_length=6)\n",
    "# split_json_filepath = Path(split_json_filepath)\n",
    "# split_json_filepath.parent.mkdir(exist_ok=True, parents=True)\n",
    "# with open(split_json_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(split_json, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.split_json(json_filepath, split_json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dirpath = 'sample/gv/json_preprocessed'\n",
    "split_json_dirpath = 'sample/gv/split_json'\n",
    "gv.split_jsons(json_dirpath, split_json_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - save duration, pitch as npy file, split audio, save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_filepath = \"sample/gv/wav/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = []\n",
    "metadata_list.append(\n",
    "    gv.preprocess_one(\n",
    "        wav_filepath,\n",
    "        split_json_filepath,\n",
    "        preprocessed_gv_path\n",
    "    )\n",
    ")\n",
    "preprocessed_gv_path = Path(preprocessed_gv_path)\n",
    "preprocessed_gv_path.mkdir(exist_ok=True, parents=True)\n",
    "with open(f\"{preprocessed_gv_path}/metadata.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(metadata_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dirpath = 'sample/gv/wav/'\n",
    "split_json_dirpath = 'sample/gv/split_json/'\n",
    "gv.save_duration_pitch_metadata_split_audio(\n",
    "    wav_dirpath,\n",
    "    split_json_dirpath,\n",
    "    preprocessed_gv_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer 사용 설명\n",
    "\n",
    "### 1. lyric_normalizer.py의 LyricNormalizer 클래스 import\n",
    "### 2. LyricNormalizer 객체 생성\n",
    "### 3. LyricNormalizer.normalize_lyrics() 함수 사용\n",
    "#### &emsp; Input: GT(whisper result), 원본 가사, pitch sequence, duration sequence\n",
    "#### &emsp; Output: 정규화 가사, pitch sequence, duration sequence, 정규화 정보를 담은 dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = SVS_Preprocessor(\n",
    "    base_path=\"preprocessed_gv\",\n",
    "    model_name=\"large-v3\",\n",
    "    device=\"cpu\",\n",
    "    language=\"ko\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/src/preprocess_svs/lyric_normalizer.py:898\u001b[39m, in \u001b[36mSVS_Preprocessor.process_all_files\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    896\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Process all files in the metadata file.\"\"\"\u001b[39;00m\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model:\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m processed_lines = []\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.metadata_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_meta:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/src/preprocess_svs/lyric_normalizer.py:624\u001b[39m, in \u001b[36mSVS_Preprocessor.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load the Whisper model.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mwhisper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/__init__.py:155\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, device, download_root, in_memory)\u001b[39m\n\u001b[32m    153\u001b[39m dims = ModelDimensions(**checkpoint[\u001b[33m\"\u001b[39m\u001b[33mdims\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    154\u001b[39m model = Whisper(dims)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    158\u001b[39m     model.set_alignment_heads(alignment_heads)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2573\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2566\u001b[39m         out = hook(module, incompatible_keys)\n\u001b[32m   2567\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m   2568\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2569\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2570\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mit should be done inplace.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2571\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2573\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[32m   2576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2561\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2555\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2556\u001b[39m         child_state_dict = {\n\u001b[32m   2557\u001b[39m             k: v\n\u001b[32m   2558\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2559\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2560\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2564\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2561\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2555\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2556\u001b[39m         child_state_dict = {\n\u001b[32m   2557\u001b[39m             k: v\n\u001b[32m   2558\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2559\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2560\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2564\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2561 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2561\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2555\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2556\u001b[39m         child_state_dict = {\n\u001b[32m   2557\u001b[39m             k: v\n\u001b[32m   2558\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2559\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2560\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2564\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2544\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[32m   2543\u001b[39m     local_metadata[\u001b[33m\"\u001b[39m\u001b[33massign_to_params_buffers\u001b[39m\u001b[33m\"\u001b[39m] = assign\n\u001b[32m-> \u001b[39m\u001b[32m2544\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2548\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module._modules.items():\n\u001b[32m   2554\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2450\u001b[39m, in \u001b[36mModule._load_from_state_dict\u001b[39m\u001b[34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[39m\n\u001b[32m   2448\u001b[39m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[32m   2449\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2450\u001b[39m             \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2451\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m   2452\u001b[39m     action = \u001b[33m\"\u001b[39m\u001b[33mswapping\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcopying\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "preprocessor.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Dataset Consistency Verification ===\n",
      "\n",
      "=== Verification Results ===\n",
      "\n",
      "No errors found!\n",
      "\n",
      "No warnings!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'errors': [], 'warnings': []}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.verify_dataset_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply G2pk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n"
     ]
    }
   ],
   "source": [
    "file_path = 'preprocessed_gv/metadata.txt'\n",
    "ps.g2p_metadata(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
