{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import whisper\n",
    "from rich import print as rprint\n",
    "import midii\n",
    "\n",
    "import preprocess_svs as ps\n",
    "from preprocess_svs import gv\n",
    "from preprocess_svs import LyricNormalizer, SVS_Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GV File Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv_path = \"D:/dataset/177.다음색 가이드보컬 데이터\"\n",
    "gv_json_sample = \"sample/gv/json\"\n",
    "gv_mid_sample = \"sample/gv/midi\"\n",
    "gv_sample_preprocessed = \"sample/gv/json_preprocessed\"\n",
    "gv_json_time_adjusted = \"D:/dataset/다음색 가이드보컬 데이터 time_adjusted\"\n",
    "gv_json_preprocessed = \"D:/dataset/다음색 가이드보컬 데이터 json preprocessed\"\n",
    "midi_filepath = \"sample/gv/midi/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.mid\"\n",
    "time_adjusted_json_filepath = \"sample/gv/json_time_adjusted/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\"\n",
    "filled_time_gaps_json_filepath = \"sample/gv/json_filled_time_gaps/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(list(ps.get_files(gv_path, \"mid\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(857142, 790)]\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "mid = midii.MidiFile(midi_filepath, convert_1_to_0=True)\n",
    "tempo_rank = mid.tempo_rank()\n",
    "print(tempo_rank)\n",
    "print(ps.calculate_top_tempo_percentage(tempo_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Tempo Deviation\n",
    "\n",
    "- json 을 처리하려면 quantize 를 위한 tempo 가 필요한데 json 에는 tempo 정보가 없음 \n",
    "- -> tempo rank 검사 \n",
    "- -> tempo 가 변하지 않는다는 충분한 보장\n",
    "- -> dominate tempo 를 채택하여 quantize 해도 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps.tempo_statistics(gv_path, parallel=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- -> 이전 end_time 이 현재 start_time 보다 큰 경우가 있음 \n",
    "- -> 이전 end_time 에 현재 start_time 을 맞추면, 뒤따라오는 메시지들의 sync 가 다 틀어짐 \n",
    "- -> 이전 end_time 을 현재 start_time 에 맞춰주는 게 더 나음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify notes sorted by time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gv.verify_json_notes_sorted_by_time(gv_path, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_note_times_sample():\n",
    "    gv_path = \"sample/gv/json\"\n",
    "    for json_path in ps.get_files(gv_path, \"json\"):\n",
    "        p_orig = Path(json_path)\n",
    "        out_path = p_orig.parent.parent / \"json_time_adjusted\" / p_orig.name\n",
    "        out_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        print(f\"adjust time of \\n{json_path}\")\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        notes = data.get(\"notes\")\n",
    "        processed_notes = gv.adjust_note_times(notes)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(processed_notes, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"saved to \\n{out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjust time of \n",
      "sample/gv/json/SINGER_66_30TO49_HUSKY_MALE_DANCE_C2835.json\n",
      "saved to \n",
      "sample/gv/json_time_adjusted/SINGER_66_30TO49_HUSKY_MALE_DANCE_C2835.json\n",
      "adjust time of \n",
      "sample/gv/json/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n",
      "saved to \n",
      "sample/gv/json_time_adjusted/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n"
     ]
    }
   ],
   "source": [
    "adjust_note_times_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill silence note between notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample/gv/json_time_adjusted/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n",
      "sample/gv/json_filled_time_gaps/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n"
     ]
    }
   ],
   "source": [
    "print(time_adjusted_json_filepath)\n",
    "print(filled_time_gaps_json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save:\n",
      "sample/gv/json_filled_time_gaps/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\n"
     ]
    }
   ],
   "source": [
    "gv.fill_time_gaps_save(time_adjusted_json_filepath, filled_time_gaps_json_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify correspondence json vs wav vs mid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">([]</span>, <span style=\"font-weight: bold\">[])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">([]</span>, <span style=\"font-weight: bold\">[])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">([]</span>, <span style=\"font-weight: bold\">[])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jsons = ps.get_files(gv_path, \"json\", sort=True)\n",
    "mids = ps.get_files(gv_path, \"mid\", sort=True)\n",
    "wavs = ps.get_files(gv_path, \"wav\", sort=True)\n",
    "rprint(gv.verify_files_coherent(jsons, mids))\n",
    "rprint(gv.verify_files_coherent(wavs, mids))\n",
    "rprint(gv.verify_files_coherent(jsons, wavs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove abnormal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gv.remove_abnormal_file(gv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GV Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filepath = \"sample/gv/json_preprocessed/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\"\n",
    "split_json_filepath = \"sample/gv/split_json/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.json\"\n",
    "preprocessed_gv_path = \"preprocessed_gv/\"\n",
    "preprocessed_gv_duration_path = \"preprocessed_gv/duration\"\n",
    "preprocessed_gv_pitch_path = \"preprocessed_gv/pitch\"\n",
    "preprocessed_gv_wav_path = \"preprocessed_gv/wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - preprocess gv json\n",
    "\n",
    "- gv json -> adjust note times + fill time gaps + quantization + frames\n",
    "- embed coherent json format(sharing with mssv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample/gv/json sample/gv/midi sample/gv/json_preprocessed\n"
     ]
    }
   ],
   "source": [
    "print(gv_json_sample, gv_mid_sample, gv_sample_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.preprocess_json(\n",
    "    gv_json_sample, gv_mid_sample, gv_sample_preprocessed, parallel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gv.preprocess_json(\n",
    "#     gv_path,\n",
    "#     gv_path,\n",
    "#     gv_json_preprocessed,\n",
    "#     parallel=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - split notes by silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_json = ps.split_json_by_silence(json_filepath, min_length=6)\n",
    "# split_json_filepath = Path(split_json_filepath)\n",
    "# split_json_filepath.parent.mkdir(exist_ok=True, parents=True)\n",
    "# with open(split_json_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(split_json, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.split_json(json_filepath, split_json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dirpath = 'sample/gv/json_preprocessed'\n",
    "split_json_dirpath = 'sample/gv/split_json'\n",
    "gv.split_jsons(json_dirpath, split_json_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - save duration, pitch as npy file, split audio, save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_filepath = \"sample/gv/wav/SINGER_16_10TO29_CLEAR_FEMALE_BALLAD_C0632.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = []\n",
    "metadata_list.append(\n",
    "    gv.preprocess_one(\n",
    "        wav_filepath,\n",
    "        split_json_filepath,\n",
    "        preprocessed_gv_path\n",
    "    )\n",
    ")\n",
    "preprocessed_gv_path = Path(preprocessed_gv_path)\n",
    "preprocessed_gv_path.mkdir(exist_ok=True, parents=True)\n",
    "with open(f\"{preprocessed_gv_path}/metadata.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(metadata_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dirpath = 'sample/gv/wav/'\n",
    "split_json_dirpath = 'sample/gv/split_json/'\n",
    "gv.save_duration_pitch_metadata_split_audio(\n",
    "    wav_dirpath,\n",
    "    split_json_dirpath,\n",
    "    preprocessed_gv_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer 사용 설명\n",
    "\n",
    "### 1. lyric_normalizer.py의 LyricNormalizer 클래스 import\n",
    "### 2. LyricNormalizer 객체 생성\n",
    "### 3. LyricNormalizer.normalize_lyrics() 함수 사용\n",
    "#### &emsp; Input: GT(whisper result), 원본 가사, pitch sequence, duration sequence\n",
    "#### &emsp; Output: 정규화 가사, pitch sequence, duration sequence, 정규화 정보를 담은 dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = SVS_Preprocessor(\n",
    "    base_path=\"preprocessed_gv\",\n",
    "    model_name=\"large-v3\",\n",
    "    device=\"cpu\",\n",
    "    language=\"ko\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in DTW: gt_text: 처음엔너일거라생각못했어\n",
      "raw_text: 모오스읍처어으매앤너어이일거라아새앵각모오테써어\n",
      "DTW 행렬의 위치 [0, 13]에서 잘못된 경로 정보를 발견했습니다. 2개의 정수로 구성된 튜플이 필요하지만, 다음을 받았습니다: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in DTW: gt_text: 내가아는그녀와있을네모습\n",
      "raw_text: 내애가아아니인그으녀어와이쓰을니이모오스읍\n",
      "DTW 행렬의 위치 [0, 13]에서 잘못된 경로 정보를 발견했습니다. 2개의 정수로 구성된 튜플이 필요하지만, 다음을 받았습니다: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in DTW: gt_text: 우린다갈수있나왜또너는내게왜이리달콤해\n",
      "raw_text: 우우리인다아가알쑤우이있나아왜애또오너어는내애게에왜이리이달코옴해애왜애\n",
      "DTW 행렬의 위치 [0, 32]에서 잘못된 경로 정보를 발견했습니다. 2개의 정수로 구성된 튜플이 필요하지만, 다음을 받았습니다: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ccsss/repo/preprocess-svs-dataset/.venv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing normalized metadata to: preprocessed_gv/metadata.txt\n",
      "Done.\n",
      "\n",
      "Starting dataset verification...\n",
      "\n",
      "=== Starting Dataset Consistency Verification ===\n",
      "\n",
      "=== Verification Results ===\n",
      "\n",
      "No errors found!\n",
      "\n",
      "No warnings!\n",
      "\n",
      "Dataset verification completed successfully!\n"
     ]
    }
   ],
   "source": [
    "preprocessor.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Dataset Consistency Verification ===\n",
      "\n",
      "=== Verification Results ===\n",
      "\n",
      "No errors found!\n",
      "\n",
      "No warnings!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'errors': [], 'warnings': []}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.verify_dataset_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply G2pk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n",
      "mecab installed\n"
     ]
    }
   ],
   "source": [
    "file_path = 'preprocessed_gv/metadata.txt'\n",
    "ps.g2p_metadata(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
